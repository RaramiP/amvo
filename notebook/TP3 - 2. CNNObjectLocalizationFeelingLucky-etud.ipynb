{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a251ed",
   "metadata": {},
   "source": [
    "# Exercice 2. Detection d'objets en tentant sa chance ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31a8ad-8de9-48ce-bc7e-31b89ebe3e5a",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons générer aléatoirement une série de régions. Pour chaque région, nous envisagerons de la faire passer par le classificateur d'objets entraîné (mono- ou multi-catégories) dans l'exercice **1**. Il n'est pas nécessaire de réentraîner le réseau. Effectuez simplement une prediction de la présence ou l'absence d'un objet dans la regions.\n",
    "\n",
    "Lors de la génération aléatoire des régions, vous pouvez demander la génération de plusieurs types de regions (rectangles debouts, carrés, rectangles couchés). Vous obtiendrez ainsi pour chaque image un ensemble de regions caractérisées par: (x<sub>i</sub>,y<sub>i</sub>,w<sub>i</sub>,h<sub>i</sub>).\n",
    "\n",
    "**Q1** Ecrire le code de la function qui génére les regions candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f05fe7-a101-47e9-94b1-6ef765ba0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2245279a-e77e-4703-bcdc-5930cce973ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preselectRegions(image, regions, classifier, input_size=(128,128), threshold=0.5):\n",
    "    \"\"\"\n",
    "    Préselectionne les régions potentiellement contenant un objet.\n",
    "\n",
    "    Args:\n",
    "        image: image originale (H,W,3)\n",
    "        regions: liste de tuples (x,y,w,h)\n",
    "        classifier: modèle CNN entraîné (binaire ou multiclasse)\n",
    "        input_size: taille à laquelle redimensionner les régions avant prediction\n",
    "        threshold: seuil de confiance pour garder une région (pour binaire)\n",
    "\n",
    "    Returns:\n",
    "        preselected: liste de [[x, y, w, h, c, logit], ...]\n",
    "    \"\"\"\n",
    "    preselected = []\n",
    "\n",
    "    for (x, y, w, h) in regions:\n",
    "        # Extraire la région et redimensionner\n",
    "        region_crop = image[y:y+h, x:x+w]\n",
    "        region_resized = cv2.resize(region_crop, input_size)\n",
    "        region_resized = np.expand_dims(region_resized, axis=0).astype(np.float32)\n",
    "\n",
    "        # Normalisation si nécessaire (MobileNet)\n",
    "        if hasattr(classifier.layers[1], 'weights'):  # détection MobileNet\n",
    "            region_resized = preprocess_input(region_resized)\n",
    "\n",
    "        # Prédiction\n",
    "        logits = classifier(region_resized, training=False).numpy()[0]\n",
    "\n",
    "        if logits.size == 1:\n",
    "            # Binaire\n",
    "            prob = logits[0]\n",
    "            if prob >= threshold:\n",
    "                preselected.append([x, y, w, h, 1, prob])\n",
    "        else:\n",
    "            # Multiclasse\n",
    "            c = int(np.argmax(logits))\n",
    "            logit_c = logits[c]\n",
    "            preselected.append([x, y, w, h, c, logit_c])\n",
    "\n",
    "    return preselected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95215a33-e3bf-45d2-bc3f-045a00d6c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_regions(img_shape, n_regions=10, min_size_ratio=0.1, max_size_ratio=0.5):\n",
    "    H, W = img_shape[:2]\n",
    "    regions = []\n",
    "\n",
    "    for _ in range(n_regions):\n",
    "        # choisir le type de région : carré, rectangle debout, rectangle couché\n",
    "        region_type = random.choice(['square', 'portrait', 'landscape'])\n",
    "        \n",
    "        # largeur et hauteur aléatoires\n",
    "        size_ratio = random.uniform(min_size_ratio, max_size_ratio)\n",
    "        if region_type == 'square':\n",
    "            w = h = int(size_ratio * min(W, H))\n",
    "        elif region_type == 'portrait':  # rectangle debout\n",
    "            w = int(size_ratio * W * 0.6)\n",
    "            h = int(size_ratio * H * 1.2)\n",
    "        else:  # 'landscape' rectangle couché\n",
    "            w = int(size_ratio * W * 1.2)\n",
    "            h = int(size_ratio * H * 0.6)\n",
    "        \n",
    "        # limiter la taille pour rester dans l'image\n",
    "        w = min(w, W)\n",
    "        h = min(h, H)\n",
    "        \n",
    "        # position aléatoire\n",
    "        x = random.randint(0, W - w)\n",
    "        y = random.randint(0, H - h)\n",
    "        \n",
    "        regions.append((x, y, w, h))\n",
    "    \n",
    "    return regions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87126429-5237-4313-96c7-54017d4886e1",
   "metadata": {},
   "source": [
    "Vous passerez chacune de ces regions par le réseau (de l'exercice **1**) et vous conserverez uniquement celles pour lesquelles un objet a été reconnu.\n",
    "\n",
    "Conserver la classe et la valeur du logit correspondant qui nous servira à évaluer la confiance en la detection. Ainsi, pour chaque image vous avez une nouvelle description des regions (x<sub>i</sub>,y<sub>i</sub>,w<sub>i</sub>,h<sub>i</sub>,c<sub>i</sub>,logit<sub>i</sub>) où c<sub>i</sub> correspond à la classe et logit<sub>i</sub> au logit ayant déterminé la classe (il jouera le rôle de la confiance qu'on fait en la detection).\n",
    "\n",
    "**Q2** Ecrire le code de la fonction qui preselectionne les régions contenant un objet potentiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4be0c29-4f04-4cfd-9af8-3bd4aab58cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preselectRegions(image, regions, classifier, input_size=(128,128), threshold=0.5):\n",
    "    preselected = []\n",
    "\n",
    "    for (x, y, w, h) in regions:\n",
    "        # Extraire la région et redimensionner\n",
    "        region_crop = image[y:y+h, x:x+w]\n",
    "        region_resized = cv2.resize(region_crop, input_size)\n",
    "        region_resized = np.expand_dims(region_resized, axis=0).astype(np.float32)\n",
    "\n",
    "        # Normalisation si nécessaire (MobileNet)\n",
    "        if hasattr(classifier.layers[1], 'weights'):  # détection MobileNet\n",
    "            region_resized = preprocess_input(region_resized)\n",
    "\n",
    "        # Prédiction\n",
    "        logits = classifier(region_resized, training=False).numpy()[0]\n",
    "\n",
    "        if logits.size == 1:\n",
    "            # Binaire\n",
    "            prob = logits[0]\n",
    "            if prob >= threshold:\n",
    "                preselected.append([x, y, w, h, 1, prob])\n",
    "        else:\n",
    "            # Multiclasse\n",
    "            c = int(np.argmax(logits))\n",
    "            logit_c = logits[c]\n",
    "            preselected.append([x, y, w, h, c, logit_c])\n",
    "\n",
    "    return preselected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e69b5a-cf4a-4c04-8de3-f8acb5a2251e",
   "metadata": {},
   "source": [
    "Pour les régions classées comme contenant un objet, appliquez l'algorithme de suppression des non-maxima classe par classe.\n",
    "\n",
    "Triez les régions par ordre décroissant de score de confiance/prédiction.\n",
    "\n",
    "Calculez l'IoU entre la première région de la liste et toutes les autres. Si l'IoU est supérieur au seuil, supprimez la région correspondante de la liste.\n",
    "\n",
    "Répétez l'opération jusqu'à ce que toutes les régions aient été traitées ou supprimées.\n",
    "\n",
    "**Q3** Proposez une implementation pour le calcul de l'IoU et pour l'algorithm de NonMaxSuppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86ffbb94-9475-4d11-99f3-2c2b49980cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(regionA, regionB):\n",
    "    xA = max(regionA[0], regionB[0])\n",
    "    yA = max(regionA[1], regionB[1])\n",
    "    xB = min(regionA[0] + regionA[2], regionB[0] + regionB[2])\n",
    "    yB = min(regionA[1] + regionA[3], regionB[1] + regionB[3])\n",
    "\n",
    "    # intersection width/height\n",
    "    inter_w = max(0, xB - xA)\n",
    "    inter_h = max(0, yB - yA)\n",
    "    inter_area = inter_w * inter_h\n",
    "\n",
    "    # union\n",
    "    areaA = regionA[2] * regionA[3]\n",
    "    areaB = regionB[2] * regionB[3]\n",
    "    union_area = areaA + areaB - inter_area\n",
    "\n",
    "    if union_area == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return inter_area / union_area\n",
    "\n",
    "        \n",
    "def nonmaxSuppression(image, classes, regionsWithClassAndLogit, iou_threshold=0.5):\n",
    "    selected_regions = []\n",
    "\n",
    "    for cls in classes:\n",
    "        # extraire les régions de cette classe\n",
    "        cls_regions = [r for r in regionsWithClassAndLogit if r[4] == cls]\n",
    "\n",
    "        # trier par logit décroissant\n",
    "        cls_regions = sorted(cls_regions, key=lambda r: r[5], reverse=True)\n",
    "\n",
    "        keep = []\n",
    "        while cls_regions:\n",
    "            # prendre la première région\n",
    "            current = cls_regions.pop(0)\n",
    "            keep.append(current)\n",
    "\n",
    "            # supprimer toutes les régions qui se chevauchent trop\n",
    "            cls_regions = [r for r in cls_regions if IoU(current, r) <= iou_threshold]\n",
    "\n",
    "        selected_regions.extend(keep)\n",
    "\n",
    "    return selected_regions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e50c7-cd95-4bd1-aae5-b2aeebd07eeb",
   "metadata": {},
   "source": [
    "**Q4** Calculez ensuite, pour chaque région restante, l'IoU avec les boîtes englobantes consituant la vérité de terrain et gardez uniquement les fenetre avec un IoU supperieur à un seuil fixé. Parcourez les regions en fonction des leur niveau de confiance. Au fur et à mesure que les groundtruth_regions se voient attribuer une candidate_region, il faut enlever cette groundtruth_region de la liste afin d'éviter que plusieurs regions candidates soient ratachées à la même région de la groundtruth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c10c13cd-b0e1-4915-98bc-92d6214281c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareToGroundTruth(image, groundtruth_regions, candidate_regions, iou_threshold=0.5):\n",
    "    # Trier les candidates par logit décroissant\n",
    "    candidates_sorted = sorted(candidate_regions, key=lambda r: r[5], reverse=True)\n",
    "\n",
    "    TPRegions = []\n",
    "    FNDetected = []\n",
    "    # Faire une copie des ground-truth pour enlever au fur et à mesure\n",
    "    remaining_gt = groundtruth_regions.copy()\n",
    "\n",
    "    for cand in candidates_sorted:\n",
    "        matched = False\n",
    "        best_iou = 0\n",
    "        best_gt = None\n",
    "        for gt in remaining_gt:\n",
    "            iou = IoU(cand, gt)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt = gt\n",
    "\n",
    "        if best_iou >= iou_threshold:\n",
    "            TPRegions.append(cand)\n",
    "            remaining_gt.remove(best_gt)\n",
    "            matched = True\n",
    "        else:\n",
    "            FNDetected.append(cand)\n",
    "\n",
    "    # Les gt restant sont les FN non détectés\n",
    "    FNNonDetected = remaining_gt\n",
    "\n",
    "    return TPRegions, FNDetected, FNNonDetected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a4559-567c-4aa0-a503-38418e5c601a",
   "metadata": {},
   "source": [
    "## Détection d'une classe d'objets\n",
    "\n",
    "Appliquez la procédure à l'ensemble d'images contenant l'étiquette 4 (Éléphant). \n",
    "Nous ne découperons plus par rapport à l'éléphant, mais considérons l'image dans son ensemble. \n",
    "Dans cette situation, nous considérons que tout autre objet apparaîssant sur les images que l'éléphant fait partie de l'arrière plan. \n",
    "Lors du chargement des données, uniquement les regions concernant l'éléphant feront partie de la vérité de terrain.\n",
    "\n",
    "Utilisez un classifier binaire capable de reconnaître un éléphant.\n",
    "\n",
    "Vous devez coder une nouvelle version de la fonction `load_elephant_objects` afin de prendre en compte ces considérations.\n",
    "\n",
    "**Q5** Coder `load_elephant_objects`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3796ee3d-3cf7-4103-90ca-3a331bd1e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_elephant_objects(imgs_path, max_samples=None):\n",
    "    x = []\n",
    "    y = []\n",
    "    count_samples = 0\n",
    "\n",
    "    if max_samples is None:\n",
    "        max_samples = np.iinfo(np.int32).max  # valeur maximale par défaut\n",
    "\n",
    "    imgs_files = os.listdir(imgs_path + \"images/\")\n",
    "    for i, img_file in enumerate(imgs_files):\n",
    "        if count_samples >= max_samples:\n",
    "            break\n",
    "\n",
    "        label_file = img_file[:-4] + \".txt\"\n",
    "        img_init = cv2.imread(imgs_path + \"images/\" + img_file)\n",
    "        if img_init is None:\n",
    "            continue\n",
    "\n",
    "        labels = csv.reader(open(imgs_path + \"labels/\" + label_file, \"r\"), delimiter=' ')\n",
    "        rows = list(labels)\n",
    "\n",
    "        elephant_boxes = []\n",
    "        for row in rows:\n",
    "            if row[0] != '4':  # garder uniquement les éléphants\n",
    "                continue\n",
    "\n",
    "            bbox = np.array(row[1:], dtype=np.float32)\n",
    "            w = int(bbox[2] * img_init.shape[1])\n",
    "            h = int(bbox[3] * img_init.shape[0])\n",
    "            x0 = max(0, int(bbox[0] * img_init.shape[1] - w / 2))\n",
    "            y0 = max(0, int(bbox[1] * img_init.shape[0] - h / 2))\n",
    "            x1 = min(img_init.shape[1], int(x0 + w))\n",
    "            y1 = min(img_init.shape[0], int(y0 + h))\n",
    "\n",
    "            elephant_boxes.append([x0, y0, x1 - x0, y1 - y0])\n",
    "\n",
    "        if elephant_boxes:\n",
    "            x.append(img_init)\n",
    "            y.append(elephant_boxes)\n",
    "            count_samples += 1\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "374da53e-6f3b-4042-ab58-02ea4d832b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/toy-dataset\"\n",
    "train_path = os.path.join(data_path, \"train/\")\n",
    "test_path = os.path.join(data_path, \"test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01fc53b5-8203-4ad5-aafe-1013dd9030ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images test contenant des éléphants : 30\n",
      "Image 0 : 1 éléphant(s) - boxes : [[307, 34, 52, 62]]\n",
      "Image 1 : 1 éléphant(s) - boxes : [[316, 43, 40, 90]]\n",
      "Image 2 : 1 éléphant(s) - boxes : [[137, 31, 43, 73]]\n",
      "Image 3 : 1 éléphant(s) - boxes : [[285, 150, 35, 88]]\n",
      "Image 4 : 1 éléphant(s) - boxes : [[188, 193, 45, 89]]\n"
     ]
    }
   ],
   "source": [
    "# Charger les images de test contenant des éléphants\n",
    "x_test, y_test = load_elephant_objects(test_path)  # test_path = \"../data/toy-dataset/test/\"\n",
    "\n",
    "print(\"Nombre d'images test contenant des éléphants :\", len(x_test))\n",
    "for i, boxes in enumerate(y_test[:5]):\n",
    "    print(f\"Image {i} : {len(boxes)} éléphant(s) - boxes :\", boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dca53d-dccb-4dbb-8ccb-e08cb50d2a5c",
   "metadata": {},
   "source": [
    "**Q6** Pour chaque image du test set affichez le nombre de TP, FP, TN.\n",
    "Choissisez un sous-ensemble de 3 images sur lesquelles vous superposer en vert la GT, en blue les TP, en orange les FP et en rouge les FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0bfcb85-409f-4859-ae7d-d1ddafaf34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image(image, groundtruth_boxes, candidate_regions, iou_threshold=0.5):\n",
    "    TP, FP, FN = compareToGroundTruth(image, groundtruth_boxes, candidate_regions, iou_threshold)\n",
    "    \n",
    "    # Pour TN : dans notre cas, toutes les régions candidates non détectées et qui ne contiennent pas d'éléphant\n",
    "    TNCount = 0  # si on ne génère pas explicitement les candidates négatives, TN = 0\n",
    "    return TP, FP, FN, TNCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7b4726e-2954-49cd-a90b-a764fe3f7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_with_gt_tp_fp_fn(image, groundtruth_boxes, TPRegions, FPRegions, FNNonDetected):\n",
    "    img_disp = image.copy()\n",
    "\n",
    "    # Ground Truth - vert\n",
    "    for r in groundtruth_boxes:\n",
    "        x, y, w, h = r[:4]\n",
    "        cv2.rectangle(img_disp, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "    \n",
    "    # True Positives - bleu\n",
    "    for r in TPRegions:\n",
    "        x, y, w, h = r[:4]\n",
    "        cv2.rectangle(img_disp, (x,y), (x+w, y+h), (0,0,255), 2)\n",
    "\n",
    "    # False Positives - orange\n",
    "    for r in FPRegions:\n",
    "        x, y, w, h = r[:4]\n",
    "        cv2.rectangle(img_disp, (x,y), (x+w, y+h), (255,165,0), 2)\n",
    "\n",
    "    # False Negatives - rouge\n",
    "    for r in FNNonDetected:\n",
    "        x, y, w, h = r[:4]\n",
    "        cv2.rectangle(img_disp, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(cv2.cvtColor(img_disp, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e2f3467-6059-46fd-833e-ad0d5eedee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "# Exemple : chemin vers le modèle sauvegardé\n",
    "model_binary = load_model(\"model_elephant_binary.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46e29003-f74a-49b0-bc7d-42f3cba0d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(imgs, target_resize=(128, 128)):\n",
    "    resized_imgs = []\n",
    "    for img in imgs:\n",
    "        img_resized = cv2.resize(img, target_resize)\n",
    "        resized_imgs.append(img_resized)\n",
    "    return np.array(resized_imgs)\n",
    "\n",
    "x_test = tf.stack(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f355c938-abc0-4939-9484-2b97fbd34f06",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m regions \u001b[38;5;241m=\u001b[39m generate_random_regions(img\u001b[38;5;241m.\u001b[39mshape, n_regions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 2. Préselection par CNN binaire\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m candidate_regions \u001b[38;5;241m=\u001b[39m \u001b[43mpreselectRegions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_binary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 3. NMS\u001b[39;00m\n\u001b[1;32m     12\u001b[0m classes_present \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# binaire : 1 = éléphant\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mpreselectRegions\u001b[0;34m(image, regions, classifier, input_size, threshold)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m regions:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Extraire la région et redimensionner\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     region_crop \u001b[38;5;241m=\u001b[39m image[y:y\u001b[38;5;241m+\u001b[39mh, x:x\u001b[38;5;241m+\u001b[39mw]\n\u001b[0;32m----> 7\u001b[0m     region_resized \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregion_crop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     region_resized \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(region_resized, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Normalisation si nécessaire (MobileNet)\u001b[39;00m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "iou_threshold = 0.5\n",
    "results = []\n",
    "\n",
    "for img, gt_boxes in zip(x_test, y_test):\n",
    "    # 1. Générer des régions candidates\n",
    "    regions = generate_random_regions(img.shape, n_regions=20)\n",
    "    \n",
    "    # 2. Préselection par CNN binaire\n",
    "    candidate_regions = preselectRegions(img, regions, model_binary, input_size=(224,224), threshold=0.5)\n",
    "    \n",
    "    # 3. NMS\n",
    "    classes_present = [1]  # binaire : 1 = éléphant\n",
    "    candidate_regions_nms = nonmaxSuppression(img, classes_present, candidate_regions, iou_threshold=0.5)\n",
    "    \n",
    "    # 4. Évaluation vs GT\n",
    "    TP, FP, FN = compareToGroundTruth(img, gt_boxes, candidate_regions_nms, iou_threshold=iou_threshold)\n",
    "    TN = 0  # on ne génère pas explicitement les TN ici\n",
    "    results.append([len(TP), len(FP), TN, len(FN)])\n",
    "\n",
    "# Affichage des résultats\n",
    "for i, (TPc, FPc, TNc, FNc) in enumerate(results):\n",
    "    print(f\"Image {i}: TP={TPc}, FP={FPc}, TN={TNc}, FN={FNc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5dcf7f-3e9d-4c71-8cdf-6667cb691847",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    img = x_test[i]\n",
    "    gt_boxes = y_test[i]\n",
    "\n",
    "    # Générer et filtrer les régions pour la visualisation\n",
    "    regions = generate_random_regions(img.shape, n_regions=20)\n",
    "    candidate_regions = preselectRegions(img, regions, model_binary, input_size=(224,224), threshold=0.5)\n",
    "    candidate_regions_nms = nonmaxSuppression(img, [1], candidate_regions, iou_threshold=0.5)\n",
    "    TP, FP, FN = compareToGroundTruth(img, gt_boxes, candidate_regions_nms, iou_threshold=iou_threshold)\n",
    "\n",
    "    plot_image_with_gt_tp_fp_fn(img, gt_boxes, TP, FP, FN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae1800-2838-4944-b6dc-b0c6bb18fb46",
   "metadata": {},
   "source": [
    "**Q7** Calculez la précision moyenne (AP) pour les images relevant de cette étiquette sur le test set pour un IoU de 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de54487b-5657-4e75-88c6-414f4c068830",
   "metadata": {},
   "source": [
    "Afin de calculer la précision moyenne, il est nécessaire de traiter l'ensemble des detections dans leur globalité par rapport à leur niveau de confiance.\n",
    "\n",
    "Regroupez dans une liste l'ensemble de regions candidates issues de **Q4** tout en conservant une information relative à leur appartenance aux ensembles TPregions ou FPregions. Triez cette liste en fonction de niveau de confiance et parcourez la de manière décroissante.\n",
    "\n",
    "Initialement le rappel est à zero car TP=0, FP=0 et FN=#total regions issues de la **Q4**\n",
    "Au fur et à mesure qu'on traite des regions de la liste on met à jour les valeurs cumulées TP et FP pour calculer la précion et le rappel à chaque nouvelle detection.\n",
    "\n",
    "Tracez un graphique des points (précision, rappel) intermediaires.\n",
    "\n",
    "Vous pouvez utiliser la fonction `sklearn.metrics.auc` pour calculer la valeur d'AUC en fournissant les listes avec les valeurs intermediaires de (précision et rappel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21464318-4827-4447-8e12-7284d116c232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0811335c-6c64-43f7-beaa-67cb3d0ec344",
   "metadata": {},
   "source": [
    "## Détection de plusieurs classes\n",
    "\n",
    "Appliquez la procédure à l'ensemble d'images du dataset. \n",
    "Lors du chargement des données, l'ensemble des regions concernant des objets feront partie de la vérité de terrain. Cette fois-ci l'on gardera également des informations sur la classe spécifique associée à une boîte englobante.\n",
    "\n",
    "**Q7** Vous devez coder une nouvelle version de la fonction `load_objects` afin de prendre en compte ces considérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_elephant_objects():\n",
    "    ...\n",
    "    return x,y #x ensemble d'images contenant des éléphants, y ensemble de positions des boîtes englobantes\n",
    "    #attention x=[img1,img1,img1,...], y=[[r11,r12],[r21],[r21,r22,r23],...] \n",
    "    #si l'img1 contient deux objets situés à r11 et r12\n",
    "    #si l'img2 contient un seul objet\n",
    "    #si l'img3 contient trois objets\n",
    "    #rij = [xij,yij,wij,hij,cij] où cij correspond à la classe de l'objet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e6961-4fdd-4a98-846c-7eba98416c3d",
   "metadata": {},
   "source": [
    "**Q8** Calculez la précision moyenne (AP) pour l'ensemble de données pour un IoU de 0.5.\n",
    "Par rapport au cas mono-classe, il est d'usage de calculer l'AUC classe par classe (et donc considerer les régions classe par classe) avant de calculer une moyenne qui tient compte de nombre d'instance de chaque classe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57eea60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b42ff398",
   "metadata": {},
   "source": [
    "\n",
    "**Q9** (optionnel) Pour guider mieux le processus de génération de regions candidates, vous pouvez également orienter le processus aléatoire en considérant les régions de l'image où se trouvent beaucoup de points caractéristiques (Harris, SIFTs, etc.).\n",
    "Est-ce que cela améliore les résultats ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcf412a-bd20-4451-ac2b-44a801e4358b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
