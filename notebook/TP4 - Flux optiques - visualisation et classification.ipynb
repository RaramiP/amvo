{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeaeac4a-a50e-4ecc-ac9b-8e4b92daf249",
   "metadata": {},
   "source": [
    "# TP4 - Flux optiques - visualisation et classification\n",
    "\n",
    "Dans ce TP nous travaillerons avec un sous-ensemble de vidéos du corpus `UCF Sports` que vous pouvez télécharger depuis la page Moodle du cours.\n",
    "\n",
    "Le fichier videos_samples.txt contient la liste des vidéos du corpus, ainsi que l'étiquette correspondante à chaque vidéo.\n",
    "\n",
    "Ci-dessous vous pouvez trouver quelques fonctions utiles pour la suite du TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22c5c623-dbd6-451a-8025-62cd70da75b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from descriptors import color_histogram\n",
    "\n",
    "def bgr2grayscale_numpy(img):\n",
    "    return .0722*img[:,:,0] + .7152*img[:,:,1] + .2126*img[:,:,2]\n",
    "    \n",
    "def read_file_list(input_file):\n",
    "    frame_list, label_list = [], []\n",
    "    folder = \"/\".join(input_file.split(\"/\")[:-1]) + \"/\"\n",
    "    with open(input_file) as f:\n",
    "        for l in f:\n",
    "            l = l.strip().split(' ')\n",
    "            frame_list.append(folder + l[0])\n",
    "            label_list.append(l[1])\n",
    "    return np.array(frame_list), np.array(label_list)\n",
    "\n",
    "def read_video(video_file):\n",
    "    capture = cv2.VideoCapture(video_file)\n",
    "    frames = []\n",
    "    ok, frame = capture.read()\n",
    "    if not(ok):\n",
    "        print(\"empty file \"+video_file)\n",
    "    while ok:\n",
    "        frames.append(frame[...,::-1]) # let's convert frames to RGB directly\n",
    "        ok, frame = capture.read()\n",
    "    return np.array(frames)\n",
    "\n",
    "def optical_flow_farneback(previous_frame, next_frame):\n",
    "    return cv2.calcOpticalFlowFarneback(previous_frame, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb5523-54b8-4092-858f-82d470b33d7e",
   "metadata": {},
   "source": [
    "## Caractérisation d'une vidéo au moyen des descripteurs extraits depuis chaque image de celle-ci\n",
    "\n",
    "Nous pouvons traiter une vidéo comme une séquence d'images et, par conséquent, nous pouvons voir la description de la vidéo comme étant constituée des descripteurs extraits à partir des images la composant.\n",
    "\n",
    "Dans la suite, nous nous intéressons aux descripteurs globaux tels que les histogrammes.\n",
    "\n",
    "**Q1/** Ecrire un script python qui calcule les histogrammes couleur pour chaque image d'une vidéo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b8db57-a58d-4ec4-925a-6df7f87ce613",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_path, labels = read_file_list(\"../data/ucf_sports_subset5/videos.files\")\n",
    "\n",
    "videos = []\n",
    "for path in videos_path:\n",
    "    videos.append(read_video(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f8bb76e-af47-45c5-9492-67fe01570a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_video(video):\n",
    "    hists = []\n",
    "    for frame in video:\n",
    "        hist = color_histogram(frame)\n",
    "        hists.append(hist)\n",
    "    return np.vstack(hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09922c26-db6e-4da3-a250-72de34c5957a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 512)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_video(videos[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e190d4-9b2a-4c30-bc47-cff345dfed15",
   "metadata": {},
   "source": [
    "**Q2/** Ecrire un script python qui calcule un histogramme moyen pour une vidéo, à partir des histogrammes couleur de chaque image la composant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fb989a4-de78-467a-8d27-1d26ccf388d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_hist(video):\n",
    "    hists = hist_video(video)\n",
    "    return np.mean(hists, axis=0)\n",
    "\n",
    "mean_hist(videos[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5f529-6281-4be0-94f2-20cee358d465",
   "metadata": {},
   "source": [
    "**Q3/** Si l'on considère l'histogramme moyen comme un descripteur pour une vidéo, mettez en place un protocole de classification sur les vidéos de la base `UCF Sports`. L'évaluation se fera en utilisant un processus de cross validation en 4 folds et un classifier de votre choix.\n",
    "\n",
    "Veuillez utiliser les configurations suivantes :\n",
    "\n",
    "a) utilisez que les données des classes `Diving-Side` / `Golf-Swing-Front`\n",
    "\n",
    "b) utilisez que les données des classes `Kicking-Front` / `Golf-Swing-Front`\n",
    "\n",
    "c) utilisez toutes les classes \n",
    "\n",
    "Reporter et discuter les résultats obtenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ade19413-f72e-40da-a2e2-8c8bcec0a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = []\n",
    "for video in videos:\n",
    "    mean.append(mean_hist(video))\n",
    "mean = np.array(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e214eb06-0488-4573-9aab-a632c2d51869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cross_val(desc, labels, label_use, n_folds=4):\n",
    "    idx = np.where(np.isin(labels, label_use))[0]\n",
    "    \n",
    "    X = desc[idx]\n",
    "    y = labels[idx]\n",
    "    \n",
    "    # 2. Encodage des labels (0..C-1)\n",
    "    label_map = {lab: i for i, lab in enumerate(np.unique(y))}\n",
    "    y_enc = np.array([label_map[l] for l in y])\n",
    "\n",
    "    # 3. Cross-validation stratifiée\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    accs = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y_enc)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y_enc[train_idx], y_enc[test_idx]\n",
    "\n",
    "        # 4. Classifieur\n",
    "        #clf = SVC(kernel=\"linear\", C=1)\n",
    "        clf = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # 5. Évaluation\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accs.append(acc)\n",
    "    return accs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90913c3a-f92f-4b4f-b68d-87367fc90f98",
   "metadata": {},
   "source": [
    "### Diving-Side, Golf-Swing-Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4971fce-a8b0-40bc-8323-0894bae329b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 accuracy = 0.333\n",
      "Fold 2 accuracy = 0.333\n",
      "Fold 3 accuracy = 1.000\n",
      "Fold 4 accuracy = 1.000\n",
      "\n",
      "Accuracy moyenne = 0.667 ± 0.333\n"
     ]
    }
   ],
   "source": [
    "accs = cross_val(mean, labels, [\"Diving-Side\", \"Golf-Swing-Front\"])\n",
    "for fold, acc in enumerate(accs):\n",
    "    print(f\"Fold {fold+1} accuracy = {acc:.3f}\")\n",
    "\n",
    "print(f\"\\nAccuracy moyenne = {np.mean(accs):.3f} ± {np.std(accs):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257544bd-039a-4cf6-8576-367b198854e1",
   "metadata": {},
   "source": [
    "### Kicking-Front, Golf-Swing-Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd95725c-4585-4f51-a28c-baede3380837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 accuracy = 0.333\n",
      "Fold 2 accuracy = 0.333\n",
      "Fold 3 accuracy = 0.500\n",
      "Fold 4 accuracy = 0.500\n",
      "\n",
      "Accuracy moyenne = 0.417 ± 0.083\n"
     ]
    }
   ],
   "source": [
    "accs = cross_val(mean, labels, [\"Kicking-Front\", \"Golf-Swing-Front\"])\n",
    "for fold, acc in enumerate(accs):\n",
    "    print(f\"Fold {fold+1} accuracy = {acc:.3f}\")\n",
    "\n",
    "print(f\"\\nAccuracy moyenne = {np.mean(accs):.3f} ± {np.std(accs):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d64e7-33f7-4a80-a18a-b4061596d1fd",
   "metadata": {},
   "source": [
    "### Tout les labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dafa932b-079b-453f-a8ea-4010012d8651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 accuracy = 0.353\n",
      "Fold 2 accuracy = 0.500\n",
      "Fold 3 accuracy = 0.312\n",
      "Fold 4 accuracy = 0.438\n",
      "\n",
      "Accuracy moyenne = 0.401 ± 0.073\n"
     ]
    }
   ],
   "source": [
    "accs = cross_val(mean, labels, np.unique(labels))\n",
    "for fold, acc in enumerate(accs):\n",
    "    print(f\"Fold {fold+1} accuracy = {acc:.3f}\")\n",
    "\n",
    "print(f\"\\nAccuracy moyenne = {np.mean(accs):.3f} ± {np.std(accs):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a506d438-cf1b-4679-bdba-61184a6ad9e1",
   "metadata": {},
   "source": [
    "**Q4/** Maintenant, nous allons considérer que chaque vidéo est constitués des quatres parties distinctes. Chaque partie contient un quart des images comme suit : `partie1=video[:n/4], partie2=video[n/4+1:n/2], partie3=video[2*n/4+1:3*n/4], partie4=video[3*n/4+1:]`. Désormais, nous souhaitons que le descripteur d'une vidéo soit composé de 4 histogrammes (un pour chaque partie de la vidéo). Ecrire un script python qui calcule ce nouveau descripteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcf7675a-df20-4728-9ad8-c9b156af427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(video):\n",
    "    _ , h, w, _ = video.shape\n",
    "    return [\n",
    "        video[:, :h , :w, :],\n",
    "        video[:, h: , :w, :],\n",
    "        video[:, :h , w:, :],\n",
    "        video[:, h: , w:, :],\n",
    "        \n",
    "    ]\n",
    "def descriptor_part(video):\n",
    "    parts = split(video)\n",
    "    desc = []\n",
    "    for part in parts:\n",
    "        desc.append(mea(part))\n",
    "    return np.concatenate(desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fce334bb-f1fe-499f-87ed-107f8430c89f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_hist_part' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdescriptor_part\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 14\u001b[0m, in \u001b[0;36mdescriptor_part\u001b[0;34m(video)\u001b[0m\n\u001b[1;32m     12\u001b[0m desc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts:\n\u001b[0;32m---> 14\u001b[0m     desc\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmean_hist_part\u001b[49m(part))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(desc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_hist_part' is not defined"
     ]
    }
   ],
   "source": [
    "descriptor_part(videos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15f03e-93ef-4c3c-af5d-51ec017299fb",
   "metadata": {},
   "source": [
    "**Q5/** Reprendre le protocole de la **Q3** mettez en place un protocole de classification sur les vidéos de la base `UCF Sports`. L'évaluation se fera en utilisant un processus de cross validation en 4 folds et un classifier de votre choix.\n",
    "\n",
    "Veuillez reutiliser les mêmes configurations qu'en **Q3**. Reporter et discuter les résultats obtenus en insistant sur les avantages/désavantages rapportés par ce découpage de la vidéo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd0cc6-8303-496d-bb13-ccde8fd8c13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cb93834-1d15-49fe-88e9-4222be69a3aa",
   "metadata": {},
   "source": [
    "## Visualiser les flux optiques (en tant qu'images RGB)\n",
    "\n",
    "Le flux optique permet de rendre compte du mouvement perçu dans la vidéo.\n",
    "\n",
    "Afin de s'affranchir de la spécificité de la texture/couleur des objets en mouvement, il est intéressant d'utiliser directement l'information de mouvement. Un objet rouge et un objet vert se déplaçant de la même manière, auront des histogrammes de couleurs différentes mais partagerons les mêmes propriétés de mouvement.\n",
    "\n",
    "En partant de cette observation, nous essayerons d'exploiter l'information du mouvement dans le cadre de la classification d'actions, en nous appuyant cette fois-ci sur l'information de mouvement.\n",
    "\n",
    "**Q6/** Ecrire un script python qui calcule les flux optiques entre chaque deux images successives pour une vidéo.\n",
    "\n",
    "Vous pouvez utiliser la fonction `calcOpticalFlowFarneback` avec les paramètres ci-dessous:\n",
    "```\n",
    "cv2.calcOpticalFlowFarneback(previous_frame, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "```\n",
    "\n",
    "N'oubliez pas que vous devez procéder à une conversion des images en niveaux de gris à priori. Vous pouvez utiliser la fonction `cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) ` pour cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708405cc-2c1a-4943-8d1d-1e3cec772788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40f45244-5540-4619-96dc-465cf319933c",
   "metadata": {},
   "source": [
    "**Q7/** Pour visualiser les flux optiques, nous passons par une transformation des valeurs (dx,dy) associées à un pixel à une représentation HSV où :\n",
    "    * le canal `H` à la direction du flux `atan2(dy,dx)` et\n",
    "    * le canal `V` correspond à la magnitude du flux (`la norme 2 du vecteur (dx,dy)`) normée elle-même sur l'intervalle `[0..1]`\n",
    "    \n",
    "\n",
    "```\n",
    "flow_hsv[...,0] = np.arctan2(flow[...,1], flow[...,0])/np.pi*180. + 180.\n",
    "flow_hsv[...,1] = 1\n",
    "flow_hsv[...,2] = np.linalg.norm(flow, axis=2, ord=2)\n",
    "flow_hsv[...,2] = (flow_hsv[...,2] - np.min(flow_hsv[...,2])) / (np.max(flow_hsv[...,2]) - np.min(flow_hsv[...,2]))\n",
    "```\n",
    "\n",
    "Selon que vous traitez l’ensemble des trames de flux d’une vidéo ou une trame de flux à la fois, vous devez créer le tensor `flow_hsv` de manière convenable au préalable :\n",
    "\n",
    "```\n",
    "#une trame à la fois\n",
    "flow_hsv = np.empty((flow.shape[0],flow.shape[1],3), float32)\n",
    "\n",
    "#les trames d’une video à la fois \n",
    "flows_hsv = np.empty((flow.shape[0],flow.shape[1],flow.shape[2],3), float32)\n",
    "#dans ce cas, vous devez aussi remplacer axis=2, par axis=3 lors du calcul des normes\n",
    "```\n",
    "\n",
    "L’intérêt de transformer l’ensemble des trames à la fois et d’obtenir une normalisation des flux qui tient compte de l’étendue complète des magnitudes observées sur l’intégralité de la vidéo. Sans cela, les faibles magnitudes présentes sur les trames comportant peu de mouvements seront perçues comme fortes lors de la visualisation. \n",
    "\n",
    "Nous procédons ensuite à une conversion de `HSV` vers `RGB` pour visualiser les flux ainsi obtenus. La conversion en `RGB` va générer des intensités comprises entre `[0..1]` sur chaque canal.\n",
    "\n",
    "Ecrivez une fonction qui permet de générer et sauvegarder les flux optiques sous formes d’images `RGB` pour une vidéo. Lors de la sauvegarde des images `RGB` pensez à multiplier par 255 (car valeurs comprises entre `[0..1]`) et de convertir en entiers (`int`) les valeurs contenues dans les tenseurs. Vous pouvez utiliser la fonction `np.astype` pour convertir tous les tenseurs en entiers.\n",
    "\n",
    "Pensez à utiliser le dossier `/local` pour réaliser les sauvegardes de vos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a74f4e-c0a0-408d-8a03-9411e6e1a638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "531e68a7-ba62-4ef1-a68c-646b4fec729c",
   "metadata": {},
   "source": [
    "## Vidéos - flux optiques comme orientations et magnitudes\n",
    "\n",
    "Nous pouvons aussi exploiter les flux optiques directement sans passer par une représentation `RGB` de ceux-ci.\n",
    "\n",
    "Nous considérons chaque point du flux comme un vecteur défini par son orientation et sa magnitude.\n",
    "\n",
    "L'espace de représentation des orientations est borné et s'étale entre `0°` et `359°`.\n",
    "\n",
    "En revanche, les magnitudes peuvent avoir des plages de représentation très larges, de part la vitesse de réalisation des actions, ou de part, les erreurs de mesure.\n",
    "\n",
    "Afin de pouvoir construire des histogrammes qui traitent de l'orientation et de la magnitude conjointement, nous pouvons nous appuyer sur les valeurs min et max des magnitudes observées au sein du corpus de données.\n",
    "\n",
    "**Q8/** Ecrivez une fonction qui transforme un flux optique `(dx,dy)` dans sa représentation ``orientation, magnitude)` en vous servant des règles suivantes:\n",
    "```\n",
    "flow_mo[...,0] = (np.arctan2(flow[...,1], flow[...,0])/np.pi*180. + 180.).astype(int)\n",
    "flow_mo[...,1] = np.linalg.norm(flow, axis=2, ord=2).astype(int)\n",
    "```\n",
    "Nous employons ici une conversion vers des `np.array` avec `dtype=int` afin de faciliter la construction d’histogrammes par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a636702-6557-4980-926f-f35d2822d345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05c57a48-9eeb-4762-bcd0-a97736aff7f9",
   "metadata": {},
   "source": [
    "**Q9/** Ecrivez une fonction qui extrait l'ensemble de magnitudes observées sur la première vidéo de chaque classe de mouvement et sauvegardez-les dans un fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1af33-49bf-4a32-8d54-9ac616123545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58fcbe8d-2949-40eb-9925-623ae6f4e23b",
   "metadata": {},
   "source": [
    "**Q10/** \n",
    "**a)** Ecrivez une fonction qui calcule un histogramme à 32 bins de ces magnitudes et visualisez-le afin d'identifier un seuil raisonnable pour les magnitudes apparaissant très rarement afin d'éliminer autant que possible les outliers. Les outliers corresponds souvent à des mesures abérantes causées par les erreurs se glissant lorsque les hypothèse de calcul de flux ne sont pas respectées. Ces magnitudes apparaîssent peu de fois par rapport aux magnitudes cohérentes.\n",
    "\n",
    "Vous pourriez itérer plusieurs fois en construisant des histogramme à 32 bins sur des plages de moins en moins étendue, car il se peut que lors de la première itération la plage de magnitudes soit très large pour analyser convenablement la distribution des magnitudes coherentes. \n",
    "\n",
    "**b)** Vous pourriez ensuite essayer de filtrer également les magnitudes trop petites, ne correspondant à des véritables mouvements. En construisant un  histogramme sur une plage proche de basses magnitudes (comprise en 0 et 4, par exemple), vous pourriez également essayer d'identifier à partir de quel moment il devient intéressant de considérer les flux optiques comme signifiant un véritable mouvement. Parfois, des nombreux points ne bougent pas dans l'image, mais à cause des erreurs de mesure (ouverture, discontinuités), on leur attribue des magnitudes supérieurs à zéro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b2791-aae0-4460-80f5-86c606df4534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b8e8b0b-545e-479f-8023-bd26694967df",
   "metadata": {},
   "source": [
    "**Q11/** Ecrivez une fonction qui filtre un flux optique en mettant à zéro les points dont la magnitude est inférieure au seuil minimal et supérieure au seuil maximal identifiés en **10**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a18871-d016-4fe9-892f-b2585befa6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4de0d2b5-44d6-4cbd-af8e-4145455b6878",
   "metadata": {},
   "source": [
    "**Q12/** Ecrivez une fonction qui calcule un histogramme de flux optique en partant de la représentation `(orientation, magnitude)` en ignorant les points ayant une `magnitude==0`.\n",
    "\n",
    "Pour cela vous devrez fragmenter l’espace `(orientation, magnitude)`. Si vous souhaitez disposer de `o_bins` pour fragmenter l’orientation et de `m_bins` pour fragmenter la magnitude, vous allez construire un vecteur disposant de `o_bins*m_bins` cases.\n",
    "\n",
    "L’étendue d’une cellule sera de `o_bin_etendue = 360/o_bins` degrés pour les orientations et de `m_bin_etendue = max_mag/m_bins` pour les magnitudes.\n",
    "Vous pourrez ensuite adapter la fonction color_histogram (fournie en *TP2*) pour remplir l’histogramme. \n",
    "\n",
    "Vous pouvez aussi choisir une implémentation moins efficace en parcourant les points composants le flux optiques et en incrémentant la case correspondante `bin_o*m_bins+bin_m`, où :\n",
    "* `bin_o` correspond au bin d’orientation où le point `(o,m)` devra se trouver (`=o/o_bin_etendue`) et \n",
    "* `bin_m` correspond au bin de magnitude où le point `(o,m)` devra se trouver (`=m/m_bin_etendue`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66097a67-7d78-47b4-b0f3-86fc298669b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a42b784b-8336-49d3-a3e7-43268e6d71d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "**Q13/** Calculez un histogramme moyen pour une vidéo en partant des histogramme de flux optique calculés entre deux images successives et filtré comme indiqué en **Q11**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced6f16-7904-4a1c-9d71-f81d473f18c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fa74596-cf94-49d3-bc9e-5e7e0dbc9cb3",
   "metadata": {},
   "source": [
    "**Q14/** Réappliquer les protocoles de classification de la **Q3 et Q5** sur ces nouveaux histogrammes moyens. Reporter et discuter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f14df5-b83b-45c8-81ba-925cc05a93d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51df118b-73f7-4e5a-8a72-37ea86cddfa7",
   "metadata": {},
   "source": [
    "**Q15/** Réappliquer le protocole de classification de la **Q3 et Q5** en considérant cette fois-ci uniquement des histogrammes d'orientation (en ignorant les magnitudes des flux) et en laissant également de côté les flux ayant une `magnitude == 0`. Reporter et discuter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e7b88-16e2-45c6-9e1a-2f7df0e560b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e34bd469-ac00-4cc0-902e-2cc35cad3035",
   "metadata": {},
   "source": [
    "**Q16/** Faites varier les seuils de filtrage obtenus en **Q10** et refaites les experimentations de la **Q14** (au minium 2 autres valeurs pour le seuil haut et 2 autres valeurs pour le seuil bas). Reporter et discuter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eab03a-5fa8-46b9-b235-c67a9718d49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e506d5a0-bf61-4aa1-8573-3c5a2c35ea28",
   "metadata": {},
   "source": [
    "## Vidéos - flux optiques comme images RGB (optionnel)\n",
    "\n",
    "**Q17/** A partir des images `RGB` illustrant les flux (voir **Q7**), vous pouvez réappliquer la même méthodologie décrite en **Q1**-**Q2** afin de calculer un descripteur global de la vidéo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca8c2b-8db9-40cc-a362-c60e4eee7f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89c4d82c-e508-45ac-936b-39cb413c13bc",
   "metadata": {},
   "source": [
    "**Q18/** Ecrire une fonction qui calcule un histogramme moyen pour une vidéo, à partir des histogrammes couleur de chaque image `RGB` illustrant le flux optique entre deux images successives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b1d5ce-2280-4b77-8f08-93819f991dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bc16937-d1d3-46fe-9c1b-765c55af555f",
   "metadata": {},
   "source": [
    "**Q19/** Si l'on considère l'histogramme moyen des images RGB des flux comme un descripteur pour une vidéo, mettez en place les protocoles de classification décrites dans les **Q3 et Q5**.\n",
    "\n",
    "Reporter et discuter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ba14a-0d83-4dca-83f5-15488d3fd828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
